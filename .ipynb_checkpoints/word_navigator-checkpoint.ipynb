{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36fe70d4-b205-45f4-b28f-a280e818b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m ********\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from pympler import asizeof\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f7d707-9de0-4644-b402-c8d8ea19a9d4",
   "metadata": {},
   "source": [
    "# Load list of Webster Dictionary Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc99af25-6733-4b0b-a09f-93e7eef22ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236736\n"
     ]
    }
   ],
   "source": [
    "word_list = words.words()\n",
    "word_list = [i.lower() for i in word_list]\n",
    "print(len(word_list))  # prints the number of words in the corpus\n",
    "# Ensure word_list is a set (if not already)\n",
    "word_list = set(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e3e08-836d-40f6-b08a-841723b0048e",
   "metadata": {},
   "source": [
    "# Load Google News Trained Word2Vec Embedding, then Filter to only contain Dictionary Words\n",
    "model: KeyedVector object of embeddings\n",
    "\n",
    "model_np: Numpy array of embeddings\n",
    "\n",
    "norm_model_np: Numpy array of embeddings scaled to unit length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9795800e-c6c8-4163-993b-9c35fab21d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6510956883430481), ('monarch', 0.6413194537162781), ('prince', 0.6159993410110474), ('sultan', 0.5864824056625366), ('ruler', 0.5797567367553711), ('throne', 0.5422105193138123), ('royal', 0.5239794254302979), ('kingdom', 0.5210405588150024), ('princess', 0.5161998867988586), ('King', 0.5158917903900146)]\n"
     ]
    }
   ],
   "source": [
    "# Replace 'path/to/embeddings.bin' with the actual path to your .bin file.\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "words_in_model = [word for word in model.key_to_index if word.lower() in word_list]\n",
    "#index_in_model = [model.key_to_index[i] for i in words_in_model]\n",
    "model = model[words_in_model]\n",
    "\n",
    "# Create a new KeyedVectors instance with the same vector size\n",
    "new_model = KeyedVectors(vector_size=300)\n",
    "# Assign the keys (vocabulary) in order\n",
    "new_model.index_to_key = words_in_model\n",
    "# Assign the vectors (embedding matrix)\n",
    "new_model.vectors = model\n",
    "# Rebuild the key-to-index dictionary\n",
    "new_model.key_to_index = {word: idx for idx, word in enumerate(words_in_model)}\n",
    "del(model)\n",
    "model = new_model\n",
    "model_np = model.vectors.T\n",
    "norms = np.linalg.norm(model_np, axis = 0)\n",
    "norm_model_np = model_np / norms\n",
    "\n",
    "# To get the vector for a specific word:\n",
    "vector = model['word']  # e.g., model['king']\n",
    "\n",
    "# To see the most similar words:\n",
    "similar_words = model.most_similar('king')\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee23d8-97e9-4371-a2e3-44aa5370fc67",
   "metadata": {},
   "source": [
    "# View Size of Embeddings\n",
    "1 word is 300 $\\times$ 1 vector\n",
    "\n",
    "Numpy array is 301MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbdcc65-8798-40a2-a8c9-308605d28b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 125754) model_np shape\n",
      "166814032 model size\n",
      "301809872 model_np size\n",
      "150904928 norm_model_np size\n"
     ]
    }
   ],
   "source": [
    "print(model_np.shape, \"model_np shape\")\n",
    "print(asizeof.asizeof(model), \"model size\")\n",
    "print(asizeof.asizeof(model_np), \"model_np size\")\n",
    "print(sys.getsizeof(norm_model_np), 'norm_model_np size')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c6ca21-927e-497a-8e16-66b449117066",
   "metadata": {},
   "source": [
    "# Random Selection of Words to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad60512-ba5b-4bcb-8c9c-d370c27aad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['loud','rhythm','recovery','tolerate','zero','accountant','club','form','tablet','bomber','row','lost','coin','wonder','crack','snap','reduce','hostage','season','loss','echo','interest','reform','faint','talkative','traction','slump','fine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15019196-d545-44b6-bd59-835c19439e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of model:\n",
    "\n",
    "model[key] -> key = 'word'\n",
    "model_np[:, model.key_to_index[key]] -> key = 'word'\n",
    "model_np[:, index] -> model.index_to_key[index] = 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2587903-c005-46f6-8fdb-d0c3d2304554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loud\n",
      "2.9796681\n",
      "2.9796681\n",
      "0.99999994\n",
      "rhythm\n",
      "3.5081346\n",
      "3.5081346\n",
      "1.0\n",
      "recovery\n",
      "2.9200957\n",
      "2.9200957\n",
      "1.0\n",
      "tolerate\n",
      "3.219292\n",
      "3.219292\n",
      "1.0\n",
      "zero\n",
      "2.7152338\n",
      "2.7152338\n",
      "1.0\n",
      "accountant\n",
      "3.2057168\n",
      "3.2057168\n",
      "0.99999994\n",
      "club\n",
      "2.5405009\n",
      "2.5405009\n",
      "0.99999994\n",
      "form\n",
      "2.129578\n",
      "2.129578\n",
      "1.0\n",
      "tablet\n",
      "3.5970337\n",
      "3.5970337\n",
      "1.0\n",
      "bomber\n",
      "4.021521\n",
      "4.021521\n",
      "1.0\n",
      "row\n",
      "2.6076393\n",
      "2.6076393\n",
      "0.99999994\n",
      "lost\n",
      "2.5961642\n",
      "2.5961642\n",
      "1.0\n",
      "coin\n",
      "3.2794971\n",
      "3.2794971\n",
      "1.0\n",
      "wonder\n",
      "2.5662866\n",
      "2.5662866\n",
      "1.0\n",
      "crack\n",
      "2.9027128\n",
      "2.9027128\n",
      "0.99999994\n",
      "snap\n",
      "2.795854\n",
      "2.795854\n",
      "1.0\n",
      "reduce\n",
      "2.8118677\n",
      "2.8118677\n",
      "0.99999994\n",
      "hostage\n",
      "3.9402144\n",
      "3.9402144\n",
      "0.99999994\n",
      "season\n",
      "2.5808203\n",
      "2.5808203\n",
      "1.0\n",
      "loss\n",
      "2.8209555\n",
      "2.8209555\n",
      "0.99999994\n",
      "echo\n",
      "2.7552948\n",
      "2.7552948\n",
      "1.0\n",
      "interest\n",
      "2.5428932\n",
      "2.5428932\n",
      "1.0\n",
      "reform\n",
      "3.0955439\n",
      "3.0955439\n",
      "1.0\n",
      "faint\n",
      "2.9436555\n",
      "2.9436555\n",
      "1.0\n",
      "talkative\n",
      "3.370544\n",
      "3.370544\n",
      "0.99999994\n",
      "traction\n",
      "3.2631102\n",
      "3.2631102\n",
      "1.0\n",
      "slump\n",
      "3.642595\n",
      "3.642595\n",
      "1.0\n",
      "fine\n",
      "2.6069028\n",
      "2.6069028\n",
      "0.9999999\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(i)\n",
    "    print(np.linalg.norm(model[i]))\n",
    "    print(np.linalg.norm(model_np[:,model.key_to_index[i]]))\n",
    "    print(np.linalg.norm(norm_model_np[:,model.key_to_index[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dfad6d-a5a6-4728-85f7-408ceace548e",
   "metadata": {},
   "source": [
    "# Cosine Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eaa2b2-822d-476a-9e16-6594253a264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(x,y):\n",
    "    x_norm = np.sqrt(x.T @ x)\n",
    "    y_norm = np.sqrt(y.T @ y)\n",
    "    cross = x.T @ y\n",
    "    cos = cross / (x_norm * y_norm)\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981f6439-409b-42ac-b442-cc9a562374b4",
   "metadata": {},
   "source": [
    "## Find Subspace Spanned by two Word Vectors, then Find Vectors with Lowest Distance to the Subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda6d09-a20e-4239-bef8-5270eb812403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meat_bread = model_np[:, [model.key_to_index['meat'], model.key_to_index['bread']]]\n",
    "proj = meat_bread @ np.linalg.inv(meat_bread.T @ meat_bread) @ meat_bread.T\n",
    "null_proj = np.eye(300) - proj\n",
    "smallest_dist = [1000, 1000, 1000]\n",
    "smallest_dist_word = [0, 0, 0]\n",
    "for i in range(200):\n",
    "    vecs = model_np[:, (i*100):((i+1)*100)]\n",
    "    \n",
    "    dists = np.diag(vecs.T @ (null_proj) @ vecs)\n",
    "    diag_sort = np.argsort(dists)\n",
    "    print(model.index_to_key[(i*100):((i+1)*100)][0:10])\n",
    "    print(diag_sort[0:10])\n",
    "    #global_indices = np.array(model.index_to_key[(i*100):((i+1)*100)])[diag_sort]\n",
    "    global_indices = np.arange((i*100), ((i+1)*100))\n",
    "    \n",
    "#    print(f\"Smallest: {model.index_to_key[global_indices[0]]} = {dists[diag_sort[0]]}, {model.index_to_key[global_indices[1]]} = {dists[diag_sort[1]]}, {model.index_to_key[global_indices[2]]} = {dists[diag_sort[2]]}\")\n",
    "#    print(f\"Furthest: {model.index_to_key[global_indices[-1]]} = {dists[diag_sort[-1]]}\")\n",
    "    if dists[diag_sort[0]] < smallest_dist[0]:\n",
    "        smallest_dist[2] = smallest_dist[1]\n",
    "        smallest_dist[1] = smallest_dist[0]\n",
    "        smallest_dist[0] = dists[diag_sort[0]]\n",
    "        smallest_dist_word[2] = smallest_dist_word[1]\n",
    "        smallest_dist_word[1] = smallest_dist_word[0]\n",
    "        smallest_dist_word[0] = model.index_to_key[global_indices[0]]\n",
    "\n",
    "smallest_dist_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44425caf-1a7b-41b7-9a1c-10f1b10078ff",
   "metadata": {},
   "source": [
    "### Same ish as before but with normalized vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f2a84-ae5e-4ba8-9fdc-407af933dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "meat_bread = norm_model_np[:, [model.key_to_index['add'], model.key_to_index['subtract']]]\n",
    "proj = meat_bread @ np.linalg.inv(meat_bread.T @ meat_bread) @ meat_bread.T\n",
    "null_proj = np.eye(300) - proj\n",
    "smallest_dist = [1000, 1000, 1000]\n",
    "smallest_dist_word = [0, 0, 0]\n",
    "for i in range(30000):\n",
    "    vecs = norm_model_np[:, (i*100):((i+1)*100)]\n",
    "    \n",
    "    dists = np.diag(vecs.T @ (null_proj) @ vecs)\n",
    "    diag_sort = np.argsort(dists)\n",
    "    global_indices = np.arange((i*100), ((i+1)*100))\n",
    "    \n",
    "    if dists[diag_sort[0]] < smallest_dist[0]:\n",
    "        smallest_dist[2] = smallest_dist[1]\n",
    "        smallest_dist[1] = smallest_dist[0]\n",
    "        smallest_dist[0] = dists[diag_sort[0]]\n",
    "        smallest_dist_word[2] = smallest_dist_word[1]\n",
    "        smallest_dist_word[1] = smallest_dist_word[0]\n",
    "        smallest_dist_word[0] = model.index_to_key[global_indices[0]]\n",
    "\n",
    "smallest_dist_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566860b-9f3b-42de-b3be-3630ca932059",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = model.key_to_index['fruit']\n",
    "word = 'fruit'\n",
    "smallest_cos = 000\n",
    "smallest_dist = 000\n",
    "smallest_dist_norm = 000\n",
    "word_cos = 'a'\n",
    "word_dist = 'a'\n",
    "word_dist_norm = 'a'\n",
    "for i in range(model_np.shape[1]):\n",
    "    cos = np.abs(cosine(model_np[:,word_index], model_np[:,i]))\n",
    "#    cos = np.abs(model.similarity(word, i))\n",
    "    dist = np.sum(model_np[:, word_index] - model_np[:,i])\n",
    "    dist_norm = np.sum(norm_model_np[:, word_index] - norm_model_np[:,i])\n",
    "    if cos > smallest_cos:\n",
    "        smallest_cos = cos\n",
    "        word_cos = model.index_to_key[i]\n",
    "    if dist > smallest_dist:\n",
    "        smallest_dist = dist\n",
    "        word_dist = model.index_to_key[i]\n",
    "    if dist_norm > smallest_dist_norm:\n",
    "        smallest_dist_norm = dist_norm\n",
    "        word_dist_norm = model.index_to_key[i]\n",
    "\n",
    "print(word_cos, smallest_cos)\n",
    "print(word_dist)\n",
    "print(word_dist_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b997fd-5e56-47c6-b1f2-9bd697f5a05c",
   "metadata": {},
   "source": [
    "### Same ish as before with cosine similarity instead of distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c0510a10-9113-4f4e-96ad-62672a630936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bread', 'meat', 'food']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meat_bread = norm_model_np[:, [model.key_to_index['meat'], model.key_to_index['bread']]]\n",
    "proj = meat_bread @ np.linalg.inv(meat_bread.T @ meat_bread) @ meat_bread.T\n",
    "min_cos = [0, 0, 0]\n",
    "min_cos_word = [0, 0, 0]\n",
    "for i in range(1200):\n",
    "    index_range = np.arange((i*100), ((i+1)*100))\n",
    "    vecs = norm_model_np[:, index_range]\n",
    "    vec_proj = proj @ vecs\n",
    "    norm_vec_proj = vec_proj / np.linalg.norm(vec_proj, axis = 0)\n",
    "\n",
    "    cosines = np.diag(norm_vec_proj.T @ vecs)\n",
    "    cosine_sort_order = np.argsort(cosines)\n",
    "    global_sort_order = index_range[cosine_sort_order]\n",
    "\n",
    "    if cosines[cosine_sort_order[-1]] > min_cos[0]:\n",
    "        min_cos[2] = min_cos[1]\n",
    "        min_cos[1] = min_cos[0]\n",
    "        min_cos[0] = cosines[cosine_sort_order[-1]]\n",
    "        min_cos_word[2] = min_cos_word[1]\n",
    "        min_cos_word[1] = min_cos_word[0]\n",
    "        min_cos_word[0] = model.index_to_key[global_sort_order[-1]]\n",
    "min_cos_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4cd16dbf-d83f-4614-9612-9400938bb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40476  3449 16254 19444 20452 28394 10033  2105  1593  4142]\n",
      "cat; dog; dogs; puppy; beagle; pooch; pup; kitten; pet; chihuahua; Pomeranian; feline; dachshund; schnauzer; poodle; Dog; pug; canine; collie; terrier; "
     ]
    }
   ],
   "source": [
    "meat_bread = norm_model_np[:, [model.key_to_index['cat'], model.key_to_index['dog']]]\n",
    "proj = meat_bread @ np.linalg.inv(meat_bread.T @ meat_bread) @ meat_bread.T\n",
    "vec_proj = proj @ norm_model_np\n",
    "vec_proj = vec_proj / np.linalg.norm(vec_proj, axis = 0)\n",
    "cosines = np.sum(vec_proj * norm_model_np, axis=0)\n",
    "cosine_sort_order = np.argsort(cosines)\n",
    "print(cosine_sort_order[-10:])\n",
    "for i in range(20):\n",
    "    print(model.index_to_key[cosine_sort_order[-(1+i)]], end = \"; \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6dd0e1-25e9-416b-8aef-05e877d02bfa",
   "metadata": {},
   "source": [
    "### I think this jawn is the same ish as before, aka find subspace, project each vector on subspace, then find cosine similarity between original vector and its projection, but as a function. I also tested a couple scoring functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "897e9e67-05a4-4e00-b98b-7a76fc6661f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plane_similarity(w1, w2, alpha):\n",
    "    M = norm_model_np[:, [model.key_to_index[w1], model.key_to_index[w2]]]\n",
    "    P = M @ np.linalg.inv(M.T @ M) @ M.T\n",
    "    P_corpus = P @ norm_model_np\n",
    "    P_norm_corpus = P_corpus / np.linalg.norm(P_corpus, axis = 0)\n",
    "    cosines = np.sum(P_norm_corpus * norm_model_np, axis = 0)\n",
    "    cosines_sort_order = np.argsort(cosines)\n",
    "    w1_cosines = np.sum(norm_model_np * M[:,[0]], axis = 0)\n",
    "    w2_cosines = np.sum(norm_model_np * M[:,[1]], axis = 0)\n",
    "    cosines_squ_w1_w2 = cosines**2 / (w1_cosines * w2_cosines)\n",
    "    cosines_squ_w1_w2_sort_order = np.argsort(cosines_squ_w1_w2)\n",
    "\n",
    "    score = cosines - alpha * ((w1_cosines + w2_cosines) / 2)\n",
    "    score_sort_order = np.argsort(score)\n",
    "    for i in range(20):\n",
    "        regular_word = model.index_to_key[cosines_sort_order[-(i+1)]]\n",
    "        scaled_word  = model.index_to_key[cosines_squ_w1_w2_sort_order[-(i+1)]]\n",
    "        score_word   = model.index_to_key[score_sort_order[-(i+1)]]\n",
    "        jos = [cosines[cosines_sort_order[-(i+1)]], w1_cosines[cosines_sort_order[-(i+1)]], w2_cosines[cosines_sort_order[-(i+1)]], cosines_squ_w1_w2[cosines_squ_w1_w2_sort_order[-(i+1)]]]\n",
    "        \n",
    "        print(f\"Regular: {regular_word:<20}  Scaled: {scaled_word:<20}  Score: {score_word:<20}\", jos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fc9064ac-839e-4804-838f-c77eaf397d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular: armor                 Scaled: minimize              Score: armor                [1.0, 0.3517392, 1.0, 28903.34]\n",
      "Regular: sword                 Scaled: ned                   Score: sword                [0.9999998, 1.0, 0.3517392, 21909.463]\n",
      "Regular: broadsword            Scaled: spec                  Score: knife                [0.66471547, 0.65937245, 0.31066358, 9393.199]\n",
      "Regular: scimitar              Scaled: Trisul                Score: broadsword           [0.61209047, 0.60222614, 0.314278, 9392.594]\n",
      "Regular: weaponry              Scaled: gombeen               Score: machete              [0.5976066, 0.3583247, 0.5737396, 7923.0474]\n",
      "Regular: rapier                Scaled: Cowling               Score: BUNA                 [0.59382147, 0.57975984, 0.32417756, 6835.2334]\n",
      "Regular: buckler               Scaled: Sheltered             Score: OFO                  [0.5895754, 0.5229112, 0.43885458, 5571.3423]\n",
      "Regular: breastplate           Scaled: unexercised           Score: SULA                 [0.5825524, 0.4694004, 0.48806792, 4489.9287]\n",
      "Regular: knife                 Scaled: meteoritic            Score: armored              [0.57342994, 0.55760163, 0.070881575, 4407.4976]\n",
      "Regular: armored               Scaled: KRISTI                Score: TEMPO                [0.57004374, 0.2046778, 0.5700264, 4291.8076]\n",
      "Regular: plowshare             Scaled: COLLIE                Score: cleaver              [0.5622296, 0.5501739, 0.3019228, 3714.5078]\n",
      "Regular: scabbard              Scaled: Viand                 Score: MELIA                [0.56166863, 0.52856135, 0.3637613, 3690.164]\n",
      "Regular: swordplay             Scaled: Chinee                Score: Tagal                [0.5577392, 0.5446234, 0.30412474, 3286.382]\n",
      "Regular: Sword                 Scaled: Magnolia              Score: WiLD                 [0.5546323, 0.5366479, 0.3199005, 2861.6553]\n",
      "Regular: knight                Scaled: eXalt                 Score: WAKONDA              [0.551245, 0.49581897, 0.39990824, 2831.3376]\n",
      "Regular: lances                Scaled: PRIORITY              Score: scimitar             [0.54903615, 0.5237383, 0.3384304, 2772.7751]\n",
      "Regular: kukri                 Scaled: chaffinch             Score: SER                  [0.54106325, 0.5354457, 0.26113364, 2665.4268]\n",
      "Regular: halberd               Scaled: VAI                   Score: WARM                 [0.5409611, 0.4828469, 0.3981715, 2654.5923]\n",
      "Regular: gladius               Scaled: sulcal                Score: ANDI                 [0.5367289, 0.49011436, 0.3771942, 2571.669]\n",
      "Regular: ballista              Scaled: Lumination            Score: AGRE                 [0.5357105, 0.44643247, 0.43421978, 2475.918]\n"
     ]
    }
   ],
   "source": [
    "plane_similarity('sword', 'armor', 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35748799-fc1b-44bd-9849-f7c448d47d98",
   "metadata": {},
   "source": [
    "# Find Mean Vector between two Words, then Find Third Vector that is Most Similar to the Mean Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3a5b69a5-749a-4ef8-a712-8c0763766310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1)\n",
      "joke: 0.5641732215881348\n",
      "ohio: 0.5641732215881348\n",
      "michigan: 0.4537144601345062\n",
      "tracy: 0.4050638973712921\n",
      "utah: 0.4015306830406189\n",
      "iowa: 0.3954731822013855\n",
      "florida: 0.393928200006485\n",
      "indiana: 0.3905448913574219\n",
      "charlie: 0.3876674771308899\n",
      "funny: 0.385683536529541\n",
      "jared: 0.3853279948234558\n",
      "missouri: 0.3847235441207886\n",
      "alabama: 0.3835403621196747\n",
      "pete: 0.3834690451622009\n",
      "sarah: 0.38255104422569275\n",
      "arkansas: 0.38168781995773315\n",
      "scarry: 0.38066619634628296\n",
      "donovan: 0.3804360032081604\n",
      "moron: 0.3803333640098572\n",
      "thats: 0.3787262439727783\n"
     ]
    }
   ],
   "source": [
    "def angle_similarity(w1, w2):\n",
    "    v1 = norm_model_np[:, [model.key_to_index[w1]]]\n",
    "    v2 = norm_model_np[:, [model.key_to_index[w2]]]\n",
    "    mean_vec = (v1 + v2)/2\n",
    "    print(v1.shape)\n",
    "    cosines = np.sum(norm_model_np * mean_vec, axis = 0)\n",
    "    cosines_sort_order = np.argsort(cosines)\n",
    "    for i in range(20):\n",
    "        word = model.index_to_key[cosines_sort_order[-(i+1)]]\n",
    "        cos = cosines[cosines_sort_order[-(i+1)]]\n",
    "        print(f\"{word}: {cos}\")\n",
    "angle_similarity('joke', 'ohio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a73e2b34-d91a-4a7a-b3fc-3c7b2cba48ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 13  8  6 10]\n",
      " [ 2  6  4  9 12]\n",
      " [ 0 10  5 19  2]\n",
      " [ 6  8 13 11  3]\n",
      " [18  6  3  0  4]]\n",
      "[[12]\n",
      " [ 9]\n",
      " [18]\n",
      " [10]\n",
      " [ 3]]\n"
     ]
    }
   ],
   "source": [
    "jo = np.random.randint(0,20, (5,5))\n",
    "po = np.random.randint(0,20,(5,1))\n",
    "print(jo)\n",
    "print(po)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ef7aaa82-2a1e-4d91-8248-b5a77532471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = np.sqrt(np.sum(model_np * model_np, axis = 0))\n",
    "lens.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "819dd61a-4b08-48eb-bb9a-5515cf167403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28217894, 0.3433174 , 0.43042833, ..., 8.988994  , 9.110525  ,\n",
       "       9.4089775 ], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b63014-a21c-4920-835a-a6af8220d827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
